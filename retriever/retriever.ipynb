{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retriever for prompt context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafael.sandriniguaracho/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        self.labeled_texts = None\n",
    "        self.original_texts = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_txts(folder_path, split_dict):\n",
    "        paths = json.load(open(split_dict, \"r\", errors=\"ignore\"))['train']\n",
    "        paths = [path.replace(\"dataset/unified\", folder_path) for path in paths]\n",
    "        paths = [path.replace(\"json\", \"txt\") for path in paths]\n",
    "        documents = []\n",
    "        for path in paths:\n",
    "            with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as file:\n",
    "                lines = file.readlines()\n",
    "                documents.append(lines)\n",
    "        return documents\n",
    "    \n",
    "    @staticmethod\n",
    "    def _truncate_lines(original_lines, labeled_lines, max_text_length):\n",
    "        def truncate_line(line, max_len):\n",
    "            match = re.search(\":\", line)\n",
    "            if line.endswith('\\n'):\n",
    "                line = line[:-1]\n",
    "                line = line[:match.start() + max_len]\n",
    "                line += '\\n'\n",
    "            else:\n",
    "                line = line[:match.start() + max_len]\n",
    "            return line\n",
    "\n",
    "        truncated_original_lines = []\n",
    "        truncated_labeled_lines = []\n",
    "\n",
    "        for i in range(len(labeled_lines)):\n",
    "            if len(labeled_lines[i]) > max_text_length:\n",
    "                truncated_labeled_lines.append(truncate_line(labeled_lines[i], max_text_length))\n",
    "                truncated_original_lines.append(truncate_line(original_lines[i], max_text_length))\n",
    "                continue\n",
    "            truncated_labeled_lines.append(labeled_lines[i])\n",
    "            truncated_original_lines.append(original_lines[i])\n",
    "\n",
    "        return truncated_original_lines, truncated_labeled_lines\n",
    "    \n",
    "    def generate_embeddings(self, texts_folder, labeled_texts_folder, split_dict, window_size=200, overlap_percent=70, max_line_length=100, save=True):\n",
    "        text_files = self._read_txts(texts_folder, split_dict)\n",
    "        labeled_text_files = self._read_txts(labeled_texts_folder, split_dict)\n",
    "        \n",
    "        all_labeled_text_chunks = []\n",
    "        all_original_text_chunks = []\n",
    "        all_segment_embeddings = []\n",
    "\n",
    "        step = int(window_size * (1 - overlap_percent / 100))\n",
    "        if step <= 0:\n",
    "            raise ValueError(\"overlap_percent must be less than 100\")\n",
    "\n",
    "        text_indices = tqdm(range(len(text_files)), desc=\"Generating embeddings\", unit=\"document\")\n",
    "        for i in text_indices:\n",
    "            original_lines = text_files[i]\n",
    "            labeled_lines = labeled_text_files[i]\n",
    "            num_lines = len(original_lines)\n",
    "            \n",
    "            if num_lines <= window_size:\n",
    "                starts = [0]\n",
    "            else:\n",
    "                # generate start indices ensuring a full window can be taken\n",
    "                starts = list(range(0, num_lines - window_size + 1, step))\n",
    "                # if the last window doesn't end exactly at the end, add one more window starting at the last possible index\n",
    "                if starts[-1] != num_lines - window_size:\n",
    "                    starts.append(num_lines - window_size)\n",
    "\n",
    "            for start in starts:\n",
    "                end = start + window_size\n",
    "                original_window = original_lines[start:end]\n",
    "                labeled_window = labeled_lines[start:end]\n",
    "\n",
    "                if not labeled_window:\n",
    "                    continue\n",
    "                \n",
    "                # generate embeddings for the labeled window and average them to get a single embedding\n",
    "                window_line_embeddings = self.model.encode(labeled_window, convert_to_tensor=True)\n",
    "                window_embedding = torch.mean(window_line_embeddings, dim=0)\n",
    "                \n",
    "                # crop excessively long lines\n",
    "                cropped_original_window, cropped_labeled_window = self._truncate_lines(original_window, labeled_window, max_line_length)\n",
    "                \n",
    "                original_window_text = \"\".join(cropped_original_window)\n",
    "                labeled_window_text = \"\".join(cropped_labeled_window)\n",
    "                #print(len(labeled_window_text))\n",
    "                all_original_text_chunks.append(original_window_text)\n",
    "                all_labeled_text_chunks.append(labeled_window_text)\n",
    "                all_segment_embeddings.append(window_embedding)\n",
    "        \n",
    "        self.labeled_texts = all_labeled_text_chunks\n",
    "        self.original_texts = all_original_text_chunks\n",
    "        self.embeddings = torch.stack(all_segment_embeddings) if all_segment_embeddings else None\n",
    "        \n",
    "        if save:\n",
    "            self.save(\"retriever/retriever_state.pkl\")\n",
    "        \n",
    "    def get_context(self, query_lines, top_k=1):\n",
    "        # Generate embeddings for each query line\n",
    "        line_embeddings = self.model.encode(query_lines, convert_to_tensor=True)\n",
    "        # Average the embeddings to produce a single query-level embedding\n",
    "        query_embedding = torch.mean(line_embeddings, dim=0, keepdim=True)\n",
    "        # Compute cosine similarity between the averaged query embedding and document embeddings\n",
    "        cosine_scores = util.cos_sim(query_embedding, self.embeddings.to(self.device))[0]\n",
    "        # Get the indices of the top_k most similar documents\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "        results = [(self.labeled_texts[idx], self.original_texts[idx], score.item()) for score, idx in zip(top_results[0], top_results[1])]\n",
    "        return results\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\"\n",
    "        Saves the retriever's state (documents, model name, and precomputed embeddings) to a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The file path to save the state.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            'labeled_texts': self.labeled_texts,\n",
    "            'original_texts': self.original_texts,\n",
    "            'embeddings': self.embeddings.cpu().numpy(),\n",
    "            'model_name': self.model_name\n",
    "        }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Retriever state saved to {file_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        \"\"\"\n",
    "        Loads the retriever's state from a file without recomputing embeddings.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The file path where the state is saved.\n",
    "        \n",
    "        Returns:\n",
    "            Retriever: An instance with the loaded state.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        # Create an instance with the stored model name\n",
    "        instance = cls(model_name=data['model_name'])\n",
    "        # Load the documents and embeddings from the saved state\n",
    "        instance.labeled_texts = data['labeled_texts']\n",
    "        instance.original_texts = data['original_texts']\n",
    "        instance.embeddings = torch.tensor(data['embeddings'])\n",
    "        print(f\"Retriever state loaded from {file_path}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 31/31 [00:25<00:00,  1.21document/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever state saved to retriever/retriever_state.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings for all documents in training set\n",
    "retriever = Retriever()\n",
    "retriever.generate_embeddings(texts_folder=\"dataset/unified_LLM\", labeled_texts_folder=\"dataset/html_multilabel\", split_dict=\"dataset/LLM_split_paths.json\", window_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever state loaded from retriever/retriever_state.pkl\n",
      "SHAREHOLDERS AGREEMENT\n",
      "for\n",
      "Car\n",
      "\n",
      "201\n",
      "201\n",
      "\n",
      "201\n",
      "201\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nresults = retriever.get_context(query_lines=query_lines, top_k=1)\\n\\nfor labeled_txt, original_txt, score in results:\\n    print()\\n    print(labeled_txt[:500])\\n    print(labeled_txt[-500:])\\n\\nfor labeled_txt, original_txt, score in results:\\n    print()\\n    print(original_txt[:500])\\n    print(original_txt[-500:])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieval example\n",
    "retriever = Retriever.load(\"retriever/retriever_state.pkl\")\n",
    "\n",
    "query_lines = [\n",
    "    \"CONTRAT DE CONDITION   S DE LIVRAISON STANDARD\",\n",
    "    \"Numéro de Contrat :\",\n",
    "    \"Le présent Contrat, constitué des conditions générales et des conditions particulières, est établi entre le Client et le Gestionnaire du Réseau de Distribution Gaz (GRD) désignés ci-après :\",\n",
    "    \"LE GESTIONNAIRE DU RESEAU DE DISTRIBUTION- GRD]\"]\n",
    "\n",
    "query_lines = [\n",
    "    \"{1} : ODM - SUPPLY AGREEMENT\",\n",
    "    \"{2} : BETWEEN:\",\n",
    "    \"{3} : ORGANIC PREPARATIONS INC.\",\n",
    "    \"{4} : 2nd Floor, Transpacific Haus\",\n",
    "    \"{5} : Lini Highway, Port Vila. Vanuatu\",\n",
    "    \"{6} : “the Manufacturer”\"\n",
    "]\n",
    "\n",
    "with open(\"dataset/original_unified_LLM/contract45.txt\", \"r\", encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "i = 0\n",
    "j = 143\n",
    "query = \"\".join(lines)\n",
    "query_lines = query\n",
    "\n",
    "print(query_lines[:30])\n",
    "print()\n",
    "for labeled_txt, original_txt, _ in retriever.get_context(query_lines=query.split(\"\\n\"), top_k=2):\n",
    "    print(len(labeled_txt.split(\"\\n\")))\n",
    "    print(len(original_txt.split(\"\\n\")))\n",
    "    print()\n",
    "\n",
    "\"\"\"\n",
    "results = retriever.get_context(query_lines=query_lines, top_k=1)\n",
    "\n",
    "for labeled_txt, original_txt, score in results:\n",
    "    print()\n",
    "    print(labeled_txt[:500])\n",
    "    print(labeled_txt[-500:])\n",
    "\n",
    "for labeled_txt, original_txt, score in results:\n",
    "    print()\n",
    "    print(original_txt[:500])\n",
    "    print(original_txt[-500:])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
